[ { "title": "Multi-Robot System", "url": "/posts/multi-robot-project/", "categories": "Robotic, UFBA", "tags": "robotic, ROS2, multi-robot, object manipulation, heterogeneous robots, simulator, computer vision, additive manufacturing, civil constructio, Pygame, Unity", "date": "2023-10-10 12:00:00 +0800", "snippet": "Industrial automation is undergoing significant changes due to technological advances. Although the application of robotic systems in assembly lines is a reality, civil construction has been more resistant to the adoption of these technologies. However, additive manufacturing (3D printing) has demonstrated potential to increase productivity and optimize operations on construction sites. Despite these advances, there are still specific tasks that require the presence of human workers, as current robotic systems may not be suitable to perform them. In this context, this work proposes the development of collaborative tasks in heterogeneous multi-robot environments, with an emphasis on communication and collaboration between different types of robots. To this end, a simulated environment will be developed that will allow the investigation and analysis of behavioral strategies that involve the use of computer vision to detect flaws and objects in the environment, in addition to object manipulation. The ultimate goal is to develop an adaptable system that can improve construction operations by reducing dependence on human labor for specific tasks. This study contributes to the advancement of automation in the sector, promoting greater efficiency and productivity.DevelopmentThe research aims to develop a simulated environment to explore and analyze computer vision strategies for detecting faults and relevant objects in the construction site, as well as object manipulation. The ultimate goal is to develop an adaptable system that can enhance operations in the construction industry, reducing the reliance on human labor for specific tasks. This study will contribute to the advancement of automation in the sector, promoting greater efficiency and productivity.The proposed multi-robot system for construction consists of four types of robots, each with specific functions (as shown in the figure below). The blue robot (R1) is responsible for additive manufacturing of structural components using 3D printing technology. The green robot (R2) specializes in delicate object manipulation, such as tube insertion and placement of power boxes. The yellow robot (R3) inspects the printed walls to identify possible flaws and unsafe points in the work environment. Additionally, it acts as an inspector for the restart of the printing process, providing spatial coordinates where the R1 robot should begin or resume printing the structure. Drones (R4, in pink) are employed for 3D terrain scanning and mapping, enabling precise construction planning and ensuring that the additive manufacturing robot (R1) stays within designated boundaries. The purple robot (R5) creates a support surface, including beam fixation, to facilitate proper material deposition by R1. The system can also incorporate other robots (Rn, in gray) to add specific capabilities to the environment, such as object manipulation.Operating environment and robot tasks.Effective communication and coordination among the robots are crucial for their successful operation. This includes exchanging mission status information, historical data, and mission details, requiring high data transmission bandwidth. Both synchronous and asynchronous coordination approaches are employed, ensuring efficient completion of complex tasks in the construction environment while prioritizing process efficiency and safety.To achieve the desired set of tasks, three main blocks have been defined: (1) simulation, which defines the environment using Unity for 3D implementation and Pygame for 2D implementation; (2) application, responsible for robot control, sensing, and communication; and (3) intelligence, incorporating artificial intelligence systems to be integrated with the robots.By combining collaborative multi-robot systems with additive manufacturing, this research addresses the challenges faced by the construction industry in adopting automation technologies. The study’s findings will contribute to advancements in construction automation, leading to improved efficiency and productivity in the sector. This project is part of my master’s degree in Mechatronics Engineering at the Federal University of Bahia, the thesis is entitled “HETEROGENEOUS MULTI-ROBOT SYSTEM FOR HANDLING AND INSPECTION OF OBJECTS IN THE CONSTRUCTION INDUSTRY” and is currently in progress.Development team Matheus França Herman Lepikson Project Summary Category: Multi-Robot System Start date: May/2022 Expected end date: July/2024 Total articles produced: 1 (for more, see the publications tab)" }, { "title": "Bbot - Balancing Robot", "url": "/posts/bbot-project/", "categories": "Robotic, SBR, Senai Cimatec", "tags": "robotic, ROS, Gazebo, control, CAD, biped", "date": "2021-12-10 12:00:00 +0800", "snippet": "Bbot or Balancing Robot, is a self-balancing autonomous robot project. Our goal is to build a mobile robot operated via ROS Noetic capable of balancing and moving on two wheels. In addition, he must be able to read a TAG (fiducial framework). The TAG will send the robot a target position to which it must navigate autonomously. To perform navigation, this robot must be able to create a map of where it is and locate itself there, allowing it to update its position throughout the mission and avoid obstacles while navigating to its objective.Bbot in a) extended pose b) bent pose.ArchitectureLEGThe project was started with the leg part. It was chosen to make a robot with leg joints to help balance the robot, varying the length of the leg to smooth over obstacles. In order to improve the robot’s grip on the ground, a silicone rubber tire was designed.The legs are subdivided into 3 degrees of freedom each, as follows: Wheels: robot locomotion. Lower legs: robot medial angulation. Upper legs: they promote the angulation of the robot’s base.Leg components in exploded view.BASEThe Bbot base, on the other hand, is designed to accommodate the sensors and all of their electronics. The exploded view of the base can be seen below, and it demonstrates all the parts.The shock absorbers (front and rear) were designed in case of robot failure and impact, protecting the sensitive parts of the sensors.Base components in exploded view.ImplementationUsing the Gazebo - ROS as a simulation tool for the environment and the robot, we were able to achieve the stability and teleoperation of Bbot. For this, we use the LQR controller.Not least, we managed to make the robot autonomous, using algorithms for navigation and location. After the implementation and study to validate the model with the simulation, we implemented the real robot!! The tests presented show that the robot can stabilize itself and with stand small disturbances.3D modelA preview of the 3D model of Bbot can be seen below. Detailed viewFor more details about the project, see the project website HERE. We have all the steps to create this project.Also see the following: Project sponsor: Senai CIMATEC. The lab website: Robotics &amp;amp; Autonomous Systems. GitHub: Real Bbot and Simulated Bbot.Development team Matheus França Lucas Souza Marco Reis Project Summary Category: Mobile Robotics Start date: May/2021 End date: December/2021 Total articles produced: 1 (for more, see the publications tab)" }, { "title": "Test Bench", "url": "/posts/testbench_project/", "categories": "Robotic, Senai Cimatec", "tags": "robotic, ROS, identification, control, CAD, black-box", "date": "2021-11-10 12:00:00 +0800", "snippet": "System identification is a generic term used to describe the mathematical tools and algorithms that allow building dynamic models from measured data. We can identify a system through equations of physics (called white box), we can identify systems without knowing the previous model (black box model) and we still have a method that is a middle ground between the white box and the black box, called gray box.The Test Bench project was a project related to the Bbot robot and aims to identify the model of the actuators that will be used in it. We will use the black box identification model.OperationThe operation of the bench will contain the following logic: Input: actuator command signal (PWM) and related speed. Output: torque measured by a load cell and speed measured by an infrared sensor.Below we show the parts in detail of the final model of the bench.Test bench in exploded view.The final bench has a HUB that supports Dynamixel actuators. Attached to the bench is a 7’’ touchscreen display for the Raspberry Pi 4, this gives the bench autonomy to work without the use of an external computer. The rendering of the final model can be seen below.Test bench rendering.ModificationsThe bench can be easily modified to couple other actuator models, just copy the model made in the Onshape software in this LINK and make the desired modifications.Model3D previewA preview of the Test Bench 3D model can be seen below. Real modelThe 3D project was correctly assembled, resulting in an autonomous bench for collecting models in the black box type.Test bench final.System identificationFor identification we have the following steps: Data collection: A step-type signal is sent to the actuator, which makes an effort on the load cell. The program then saves the dataset with the input, output and elapsed time. Data processing and refinement: We use some filters and signal clipping to clean the data. Model: Using the SIPPY library, we identify the system. For this, we use ARX (Autoregressive with Extra Input) as its internal model.The model can be seen in the equation below.\\[\\frac{0.0006618}{z-0.8444}, d t=0.0125\\]The following figure shows the torque input signal in orange and the model identified with the test bench in blue.Model identifiedThe 3D project was assembled correctly, resulting in a standalone bench for collecting black box models.For more details, including how to get the actuator model, see the project website link in the next section (detailed view).Detailed viewFor more details about the project, see the project website HERE. We have all the steps to create this project.Also see the following: Project sponsor: Senai CIMATEC. The lab website: Robotics &amp;amp; Autonomous Systems.Development team Matheus França Lucas Souza Marco Reis Project Summary Category: Mobile Robotics Start date: August/2021 End date: November/2021" }, { "title": "Hephaestus", "url": "/posts/hephaestus_project/", "categories": "Robotic, Humanoid, Senai Cimatec", "tags": "robotic, ROS, Gazebo, control, CAD, biped", "date": "2021-07-10 11:00:00 +0800", "snippet": "Hephaestus features a 22 Degrees of Freedom (DOF) articulated design. Its structure is based on Robotis models OP3 and OP2. In addition, Hephaestus is developed under ROS (Robot Operating System) to use various packages in the ROS ecosystem. All the technology involved and support for ROS, allow developers to focus more on advancing research and techniques in the field of robotics and computer vision.The project was part of a research on anthropomorphic robot at the institution Senai Cimatec.Hephaestus in the final CADComponentsThe diagram below shows how the robot is divided. On the head block is the stereo camera (Mynt Eye S1030) and two controllers for changing the head orientation. The arm block has 6 actuators. The spine contains the robot’s main electronics, with the controllers and the system’s power supply (lipo 11.1v), in addition to the IMU, the sensor responsible for measuring the robot’s inclination. In the leg block, 10 actuators are distributed for each leg, two of which have greater effort capacity, for the joint demand (dynamixel mx-106).Component diagramIt is possible to notice that the connections between the components are divided into power (red) and data (black). The actuators are shown only with a power “line” as they are connected in a chain, so we opted for this description in the diagram.3D modelA preview of the Hephaestus model can be seen below. Detailed viewFor more details about the project, see the project website HERE. We have all the steps to create this project.Also see the following: Project sponsor: Senai CIMATEC. The lab website: Robotics &amp;amp; Autonomous Systems.Development team Matheus França Breno Portela Marco Reis Project Summary Category: Mobile Robotics Start date: May/2021 End date: July/2021" }, { "title": "ADAM humanoid", "url": "/posts/adam/", "categories": "Robotic, Humanoid", "tags": "robotic, ROS, Gazebo, control, CAD, biped", "date": "2021-06-10 07:00:00 +0800", "snippet": "Robotics has been driven by the possibilities of using robots to help humans. The design of a humanoid robotic system aims to obtain a machine adapted to the human environment and that can perform tasks to help people with greater adaptability and ease of execution.This work features the humanoid robot Adam, which was my graduation thesis.Adam has an open architecture platform, both for simulations and for your physical model. The prototype is used as a test object and thus promoting advances in the most diverse robotics and artificial intelligence techniques. The robotics platform was developed using ROS (Robot Operating System), which is a collection of software frameworks for robot development, which provides the functionality of an operating system in a heterogeneous cluster of computers.Adam in the Fusion 360 software (CAD)In the end we managed to implement the vision plugin for the Intel Realsense sensor in the Gazebo simulator.a) Environment in gazebo, b) View of Rviz with point cloudWe were also able to implement the interface of moveit and Ikfast, performing some movements with the robot in simulation.Simulation of leg movement in: a) rviz environment, b) gazebo environmentWith the arrival of Covid-19, we decided to make the robot able to analyze thermal images and with that to make an alert signal.Adam’s temperature alert movement3D modelA preview of Adam’s 3D model can be seen below. Future work Due to COVID-19, Adam’s physical model could not be finalized. For future work, with all the lessons learned from the project, the control part will be carried out, both in simulations and in the real part.Development team Matheus França Fredson Oliveira Oberdan Pinheiro Project Summary Category: Mobile Robotics Start date: November/2019 End date: June/2021 Total articles produced: 4 (for more, see the publications tab)" }, { "title": "Warthog", "url": "/posts/warthog_project/", "categories": "Robotic, Vision, Personal", "tags": "robotic, ROS, SLAM, computer vision", "date": "2021-04-10 10:00:00 +0800", "snippet": "The idea of this project is to develop mobile robot techniques and autonomous navigation. For that, a mobile platform called Warthog was used, a robot manufactured by the company Clearpath Robotics.Purpose of the ProjectFor the movement of a mobile robot to be performed autonomously, it must be able to carry out, without the intervention of an external controller, the planning and execution of trajectories in the desired operating environment, the avoidance of obstacles and the location and mapping of environment, which is known as autonomous navigation. Autonomous navigation is essential for a robot to be able to perform a wide range of tasks, such as moving from one point to another in an unfamiliar environment.The project aims to use warthog robot for navigation, localization and mapping, to develop and train slam techniques. All this with ROS integration.Techniques developedNavigationNavigation is responsible for carrying out the movement of the robot given an objective position.Perception:The perception module is responsible for providing the robot with the ability to perceive the environment around it through data from on-board sensors.Planning:Responsible for calculating collision-free routes.Mapping:From the images of the stereo camera, the area covered by the robot is mapped.LocalizationThe robot location is performed through visual odometry. RtabmapI used a ROS package wrapper from RTAB-Map (Real-Time Appearance-Based Mapping), an RGB-D SLAM approach based on a global loop-closing detector with real-time constraints. This package can be used to generate 3D point clouds from the environment and/or to create a 2D occupancy grid map for navigation. Below you can see my experience with this package.Rtabmap with Warthog robotCodeThe project was all done in a simulation environment and all the code can be found in the repository following the link.Development team Matheus França Project Summary Category: Mobile Robotics Start date: April/2021 End date: April/2021" }, { "title": "DRoILT", "url": "/posts/droilt_project/", "categories": "Robotic, Vision, Senai Cimatec", "tags": "robotic, ROS, computer vision, PTL", "date": "2021-04-10 07:00:00 +0800", "snippet": "Many studies have been carried out with the aim of developing autonomous robots to travel along power transmission lines (PTL) to carry out inspection and repair. DRoILT is an autonomous mobile robot that moves on electrical power lines and can overcome different types of obstacles.OverviewThe electrical energy generated in Brazil has the largest source of production in hydroelectric plants, thermoelectric plants, wind farms and nuclear plants. These sources generally need to be distributed to consumers in regions far from their production, for which PTL (power transmission lines) are used. According to the ONS ((from portugues: Operador Nacional do Sistema Elétrico)) in 2019, Brazil had about 141,000 km of electric power lines, with a perspective of 185,000 km by 2023, with voltages above 100 kV. With the increase in energy supply, it is also necessary to think about the maintenance plans of these PTLs.PTL inspections in the Brazilian electrical system are still carried out by helicopters, professionals suspended in the PTL, foot patrol along the line’s trajectory or with drones. These solutions require a high level of expertise, are expensive, difficult to access and can pose risks to the professional’s life.ObjectiveThe objective of this project is the simulation of the PTL Inspection Robot called DRoILT, using the Gazebo 9 tool in the Ros Noetic framework. The objective is that the robot can inhabit the PTL and overcome obstacles autonomously, and that it can perform visual inspection of these PTLs while energized.The robot will use a navigation and simultaneous localization system for its mapping and detailed identification of obstacles present on the line.DRoILT robot analysisThe DRoILT robot was developed with three robotic arms arranged in line and driven by a set of pulleys. The robot can be divided into three modules:The left module is designed with a set of pulleys and atraction unit.DRoILT robot left module.The central module has a vertical rod and a pulley. It alsoincludes the central electronics.DRoILT robot central module.The right module is a mirror of the left module.DRoILT robot right module.The final model proposed has a approximated external dimensions of 1680 x 140 x 650 (W x W x H [mm]).DRoILT robot dimensions.With its modularity, it manages to overcome different types of obstacles.SimulationBelow, the DRoILT is shown moving on the power line in the gazebo simulation. The next video features the DRoILT robot overcoming an obstacle. Detailed viewFor more details about the project, see the project website HERE. We have all the steps to create this project.Also see the following: Project sponsor: Senai CIMATEC. The lab website: Robotics &amp;amp; Autonomous Systems.Development team Matheus França Jean Paulo Marco Reis Project Summary Category: Mobile Robotics Start date: April/2021 End date: April/2021" }, { "title": "Pan&amp;Tilt", "url": "/posts/pan-tilt/", "categories": "Vision, Senai Cimatec", "tags": "robotic, computer vision", "date": "2021-03-10 07:00:00 +0800", "snippet": "The Pan&amp;amp;Tilt project provides a ROS1 common packages for use in charuco detection (using pan and tilt to tracking) and image stitching. Could be used in simulation or in real world.The purpose of the project is to use a rgb camera with pan and tilt capabilities and a charuco board, to develop and train vision techniques, as tracking a object, camera calibration and image stitching. All this with ROS integration.Image stitchingAn image stitching output can be seen in the next figure. Although the same technique can be used in the real model, here I only present it in the simulation model. TrackingFor the tracking functionality, was printed a charuco board (a well know pattern), and with the setup built (servo motors and a camera), we could see that the setup follows the board correctly.Simulation It’s important to note that the dots are a little out of whack, that’s because I use a safe space. When it is sent to the robot that it is in the safe space, it stops its operation.Real The setup was designed by myself, and printed in 3d. The smartphone was connected by an IP camera driver, that outputs the image in a ROS topic.CodeAll code can be found in the repository by following the link.3D modelA preview of the 3d model can be seen below. Development team Matheus França Tiago Souza Marco Reis Project Summary Category: Mobile Robotics Start date: February/2021 End date: March/2021 For more about ROS, follow this link &amp;#8617; " }, { "title": "Horus", "url": "/posts/horus_project/", "categories": "Vision, Senai Cimatec", "tags": "robotic, computer vision", "date": "2021-02-10 07:00:00 +0800", "snippet": "For the proper functioning, guarantee of safety and integrity of living beings and the environment that surround them, it is essential that intelligent systems such as robots and autonomous cars, for example, are equipped with sensors of different natures. A common problem when using multiple sensors is ensuring they are all aligned and calibrated. In this project, it was proposed to test calibration methods between an RGB camera and a LiDAR using Computer Vision algorithms for the detection of a TAG (known object), taken as a calibration standard. Algorithms of 3D point clouds and RGB images were used to obtain coordinate correspondences (in both, simultaneously) in real and simulated environments.SetupTo calibrate between sensors, we first need to collect the data. As I show in the next images, this can be achieved with many sensor configurations.Horus setup 1Horus setup 2The first configuration shows us a Kinect and a regular smartphone camera. And in configuration 2 we have a Mynt Eye S 1030 sensor and a Velodyne. Both are precisely configured and have responded well to testing.FilterAfter obtaining the LiDAR data, the point clouds are pre-processed using spatial filters to delimit the field of view of interest (VoxelGrid and FrustumCulling) in order to reduce the number of analyzed points and, therefore, the time of processing. The results can be seen below, both in simulation and in a real environment.Filter in simulation. Point cloud without change (red) and pre-processed point cloud (green).Filter in real setupResults with AIMany regression methods were tested, we can use SVRPoly (Support Vector Regression, core: poly) as an example from the scikit-learn library, whose results are shown in the following figure as follows: (a) it is possible to see the arrangement of the data of test and prediction, (b) the error histogram (in pixels) between the test and predicted coordinates is represented and (c) the error in pixels between the test and predicted points is represented.(a) Data, (b) Error distribution per histogram, (c) Error per pixel.A total of 18 regressors from the scikit-learn library were tested and their results can be seen in the table below, which shows the mean squared deviation for the data sets and their regression scores in relation to the test set.RegressorsTest scoreMax error (pixel)RMSEOLS0,99383211,70ARDReg0,99373411,75BaggingRegressor0,98374617,45DecisionTreeRegressor0,95737125,62ElasticNetReg0,86357342,83ExtraTrees0,9982216,22GradientBoostingRegressor0,9974257,28Huber0,99353411,82KNNRegressor0,98254816,77MLPRegressor0,99383211,70RandomForestRegressor0,98504717,51RANSAC0,99373411,74Ridge0,99383311,69SGDReg0,97574820,88SVRLin0,99313412,04SVRPoly0,9987165,36SVRRBF0,99493010,17Theil-Sen0,99273812,45Error analysis for regressors; (RMSE: Root-mean-square deviation).The qualitative analysis of the regression results can be seen below, using the SVRPoly model, by observing the colored point clouds and the arrangement of the 3d points in the image. The color associated with each three-dimensional point is the same color as the image pixel resulting from the transformation of three-dimensional coordinates.a) Camera image with inserted LiDAR points, (b) actual data acquired with the camera, (c) Post processed data by the calibration model, (d) actual data acquired with the LiDAR.It is also possible to analyze, in the figure below, that the points projected on the camera interface contain an intensity control for the closest points (in red) and further away (in blue).Image of camera interface with intensity points and point cloud visualization in Rviz.For the first tests, Artificial Intelligence algorithms were used. With the evolution of the research, we chose to remove the AI part and focus on implementing a method that was just as efficient, but with a lower processing cost. The research will still generate an article, in which the method will be addressed in more depth.3D modelA preview of the 3D model of the Horus configuration can be seen below (a test configuration as an example, could be $N$ configurations). Development team Matheus França Tiago Souza Project Summary Category: Computer Vision Start date: November/2019 End date: February/2021 Total articles produced: 1 (for more, see the publications tab)" }, { "title": "Webots", "url": "/posts/webots_project/", "categories": "Robotic, Senai Cimatec", "tags": "mobile robotic, robotic", "date": "2020-12-10 07:00:00 +0800", "snippet": "This project was made to carry out the simulation of the Challenge related to the Robotics and Autonomous Systems Laboratory to act as an undergraduate intern at SENAI CIMATEC.The challenge was to develop an autonomous system that presents abilities to recognize the environment through sensors, providing an adequate autonomous navigation. For this we use the WeBots simulator, an open-source platform to develop robots!!The robot’s mission will be to arrive in the region with a lamp, which is next to a STOP sign. The region starts right after the sign towards the lamp. The robot must always start near the START board. The pathThe path was performed in 1 minute and 11 seconds in total. The explanation can be seen below: For the robot to stop at the indicated location, a lightsensor has been added to its extensionSlot. A maximum value has been set for the light sensor, which indicates when it should stop. The sensors have been initialized and the weights that each sensor has in relation to the robot movement selected. In the simulation loop, its values were then read and the robot started moving forward. Then, if the light sensor reading is greater than or equal to the set value, the robot stops (we arrived at the lamp). If you haven’t reached the goal yet, evaluate the weights of the sensors, deviate from the goals and go forward. Steps 5 and 6 store as assigned chosen in a vector, so that it is only later assigned to the motor pair. All code can be found in the repository by following the link.Development team Matheus França Marco Reis Project Summary Category: Mobile Robotics Start date: December/2020 End date: December/2020" }, { "title": "Sumo robot", "url": "/posts/sumo_project/", "categories": "Robotic, BrBots", "tags": "mobile robotic, robotic, CAD", "date": "2019-06-11 07:00:00 +0800", "snippet": "The BrBots team was a robotics core, that I and more 2 friends from university founded. Our go was build robots to competition and with that, develop new skills in the robotic field. The sumo robot is one of the types of competitions that we participate.Above you can see two of our robots, Anoobs and Paladin. We went to several competitions, including the biggest in all of Latin America, Robocore Winter ChallengePaladino and Anoobs sumo robotInstagramYou could see some of the competitions we participate and our work in the BrBots Instagram. View this post on Instagram A post shared by BrBots (@brbots) 3D modelA preview of Paladin 3d model can be seen below. interviewsFollow a interview given by BrBots team for the Área 1 university in this link, the interview is in Portuguese.Our team achieve the first and second place in the Cyber Fight 2017 competition, and a interview from the university could be seen in this link.500g Sumo categoryWe also participated in the 500g category of the Robocore competition. For this we created a small version of Paladin, called micro paladin.The 3D model can be seen below. Next stepTo compete at a higher level, we developed a sumo concept (3Kg category). We wanted to use magnets for more ground friction and lower sensors. As well as a more fortified housing. The side plates were made of carbon fiber.The 3d drawing can be seen just below. Development team Matheus França Fredson Oliveira Project Summary Category: Mobile Robotics Start date: November/2016 End date: June/2019" }, { "title": "Battle robot", "url": "/posts/beetle_project/", "categories": "Robotic, BrBots", "tags": "mobile robotic, robotic, CAD", "date": "2019-06-10 11:00:00 +0800", "snippet": "Since 2005, robot combat events have been organized in Brazil with several other categories such as Sumo, Line Follower, Lego Sumo, among others.The purpose of these competitions is to encourage people to develop new technologies, in addition to providing a spectacle for the entire public to watch and vibrate with the battles.My team (BrBots) decided to create a robot in the beetle weight category (1.4Kg). To learn robotics techniques and participate in another modality in robot competitions.Battle robot (larger is functional) and its miniature (smaller is not functional).InstagramYou could see some of the competitions we participate and our work in the BrBots Instagram. Ver essa foto no Instagram Uma publicação compartilhada por BrBots (@brbots) 3D modelA preview of the 3D model of the robot can be seen below. Development team Matheus França Fredson Oliveira Project Summary Category: Mobile Robotics Start date: November/2017 End date: June/2019" }, { "title": "Line Follower", "url": "/posts/line_project/", "categories": "Robotic, BrBots", "tags": "mobile robotic, robotic, CAD", "date": "2019-06-10 08:00:00 +0800", "snippet": "Line follower robots are usually one of the first things the novice robot builder wants to try out. Line followers typically detect a contrasting line laid out on the floor or a table and try to run along that line as closely as possible. These types of robots have very real applications in factory automation and other industrial settings. Lessons learned from building line followers form a valuable foundation for more advanced robotics topics, making these projects ideal for beginners.My team (BrBots) decided to create a robot in this category and participate in the Robocore competition. This gives us a deeper understanding of robotics.InstagramYou could see some of the competitions we participate and our work in the BrBots Instagram. Ver essa foto no Instagram Uma publicação compartilhada por BrBots (@brbots) Development team Matheus França Fredson Oliveira Project Summary Category: Mobile Robotics Start date: July/2017 End date: June/2019" }, { "title": "Lego Bot", "url": "/posts/lego_project/", "categories": "Robotic, BrBots", "tags": "mobile robotic, robotic, CAD", "date": "2019-06-10 08:00:00 +0800", "snippet": "Like the regular sumo category, lego sumo is a very challenging competition. The idea is to take a lego kit and build the whole robot with its parts.As we were very excited about the vast types of robot competitions, we saw a possibility to expand our knowledge in robotics. For this we tried the lego sumo robot category.BrBots Lego robotInstagramYou could see some of the competitions we participate and our work in the BrBots Instagram. Ver essa foto no Instagram Uma publicação compartilhada por BrBots (@brbots) Development team Matheus França Fredson Oliveira Project Summary Category: Mobile Robotics Start date: July/2017 End date: June/2019" }, { "title": "Learning Bot", "url": "/posts/learning_project/", "categories": "Robotic, BrBots", "tags": "mobile robotic, robotic, CAD", "date": "2018-12-10 09:00:00 +0800", "snippet": "As part of learning with the BrBots team and with the knowledge acquired with robotics, there are some opportunities to pass our knowledge on to high school students, in addition to motivating them to follow this area. There were also opportunities to participate as a judge in the Brazilian Robotics Olympics.Brazilian robotics olympics (from Portuguese: Olímpiada Brasileira de Robótica, OBR)The OBR has two modalities: Practical and Theoretical, which seek to adapt to both the public from schools that already have contact with educational robotics and the public that has never seen robotics. The activities take place through practical competitions (with robots) and theoretical tests throughout Brazil. Ver essa foto no Instagram Uma publicação compartilhada por BrBots (@brbots) Theoretical modalityTests that address problems in robotics that can be solved using tools and concepts included in the basic school curriculum, such as science, mathematics and languages.Practical modalityIn a hostile disaster environment, a fully autonomous robot developed by a team of students is given a difficult task: to rescue all victims without human interference.SchoolThe opportunity to teach robotics to elementary and high school students can be challenging but also rewarding. We can take our knowledge in robotics and also learn from young enthusiasts in this field.To motivate students and young people interested in the area, we developed low-cost robots, which can be seen below.Low cost hockey robotRobot hockey is made up of 5-minute matches. These are exciting matches between teams made up of three robots where the only objective is to score as many goals as possible, and for that to happen anything goes: there are no fouls in this game!Since an exciting sport can bring a great deal of interest, it was important to make a low-cost robot that would bring this idea to schools.Low cost battle robotJust like our 1.4 kg battle robot, the idea of the robot for schools is to entertain and teach. So we built a robot that does not cause risks, but shows in an interesting way the possibilities of robotics.We bring radio-controlled robots to schools to demonstrate the possibilities of robotics in a competition format.Radio-controlled robotsIn the next section you can see a little bit of an event where we took the competition robots from the BrBots team (Sumo category) to a local school.InstagramYou could see some of the competitions we participate and our work in the BrBots Instagram. Ver essa foto no Instagram Uma publicação compartilhada por BrBots (@brbots) 3D modelA preview of the 3D model of one of the learning robots we built can be seen below. Development team Matheus França Fredson Oliveira Project Summary Category: Mobile Robotics Start date: July/2017 End date: December/2018" } ]
